Imagine a space where algorithms are developed and deployed through inclusive, transparent, community-led documentation practices.  In such a place, what kind of dynamics would we expect to see?

Based on a reading of the critical algorithms literature, you'd think that we're just beginning to explore what such a space would look like.  Government and big, corporate ``platforms'' dominate the discussion of algorithmic governance issues\cite{crawford2016algorithm}\cite{diakopoulos2015algorithmic}\cite{gillespie2014relevance}\cite{tufekci2015algorithms}.

What if we told you that the Wikipedia\footnote{\url{https://wikipedia.org}} community has been developing algorithms---of the scary subjective type---in the open for more than a decade?  And, that there's an extensive literature about how the social and technical governance subsystems have developed in parallel.  In this field site, we see a unique set of new problems and opportunities that have yet to be encountered by the Facebooks, Twitters, and Googles of the internet.

By examining Wikipedia's algorithmic practices, we can explore research questions that are only askable after algorithmic development and adoption have become mostly \emph{inclusive, transparent, and community-led}.

In this paper, we'll discuss Wikipedia as a state-of-the-art socio-technical space for human-process and algorithmic co-development and describe an advanced technical system (ORES, the Objective Revision Evaluation Service\footnote{\url{https://ores.wikimedia.org}}\footnote{\url{http://enwp.org/:mw:ORES}}) that provides a critical intervention into this space.  ORES represents a theoretical intervention into Wikipedia's socio-technical problems and a technical probe of algorithmic transparency and crowd-sourced auditing.  We will describe the design rationale behind the ORES system, drawing from critical feminist theory and describing the surrounding technical environment using the framework of genre ecologies.  We describe the system, its impacts, and several case studies of how independent communities of volunteers have responded to AIs.

It's worth noting that a description of the ORES system is overdue for the CSCW literature.  Since our deployment in early 2015, ORES has become popular both in usage and in the scholarly literature.  Before it was enabled by default, the ORES service was manually enabled by roughly half of the population of Wikipedia editors across all languages.  Our announcement blog post\cite{halfaker2015artificial} and the base URL of the service itself have already been cited and footnoted several times in peer reviewed papers as a basis for improved modeling work (e.g. \cite{dang2016quality}), as an example of a publicly available machine prediction API (e.g. \cite{lewoniewski2017relative}), and through using the prediction models to support other analyses (e.g. \cite{rezguia2017stigmergic}).

We hope that by providing a rich description of the ORES system and some of the new technological and social interventions that ORES elicits as a technical probe\cite{hutchinson2003technology}, we can open the doors for researchers to explore this research platform\cite{terveen2014study}.  Through our case studies of emergent reflection and auditing of ORES, we hope to bring some real and practical considerations to recent calls for the audit-ability of algorithms\cite{sandvig2014auditing}.

We approach describing ORES and its context by first describing related literature around open algorithmic systems.  We then focus on the socio-technical context of Wikipedia and the design rationale that lead us to building ORES.  Next, we describe the ORES system and its usage highlighting our approach to \emph{openness} and \emph{transparency}.  Finally, we describe several case studies of interesting uses and critiques of ORES' predictions.

This work contributes to the literature around cooperative work and open peer production, primarily as a technical probe.  ORES is not a groundbreaking technology.  The models that ORES makes available to Wikipedia editors and tool developers are relatively simple machine classifiers that use methods that are described in related work.  However, the infrastructural role that ORES takes as a technical probe allows us to gain insights into the target intervention (reducing barriers in order to open up Wikipedia's quality control system to alternative views) and into practical transparency in algorithmic governance---from the model building pipeline to the roles that algorithms take in social processes.  This is a novel design strategy, and if our rationale holds, it has the potential to break a decades-long stagnation in the adaptation of articulation work in Wikipedia, one of humanity's most valuable information resources.
