\subsection{The politics of algorithms}
Algorithmic systems are playing increasingly important and central roles to the governance of many social processes where we take the roles of humans for granted\cite{gillespie2014relevance}.  In online spaces, they help us deal with information overload problems.  What search results best balance \emph{relevance} and \emph{importance}?  Which books are most \emph{related} to the ones a user likes?  In other spaces, they help financial institutions run more efficiently.  Which credit card transactions are \emph{similar to} known fraudulent transactions?  Who is least \emph{risky} to loan money to?  Software algorithms help us answer these subjective questions in realms where there's little to ground decisions except for a messy history of subjective human decisions\cite{tufekci2015algorithms}.  Algorithms that are designed to support work are changing the way that work happens through their integration into work practices and how they shift the dynamics of that work\cite{crawford2016algorithm}\cite{gillespie2014relevance}.  In this way, software algorithms gain political relevance on par with other process-mediating artifacts (e.g. as Lessig made famous, laws\cite{lessig1999code}.  This increasing relevance of algorithms in social and political life has lead to renewed focus on questions of fairness and transparency\footnote{See also \url{https://www.fatml.org/} for a conference devoted to these questions}.

In order to address power dynamics that are at play in the bias of such algorithms, several calls have been made for transparency and accountability of the algorithms that govern public life and access to resources\cite{diakopoulos2015algorithmic}\cite{sandvig2014auditing}\cite{kroll2016accountable}.  The field around effective transparency and accountability mechanisms is growing rapidly.  We are not be able to give a fair overview at this time due to the scale of concerns and the rapid shifting in the field, but we've found inspiration in the discussion of the potential and limitations of auditing and transparency by Kroll et al.\cite{kroll2016accountable}

In this paper, we're exploring a specific political context (Wikipedia's algorithmic quality control and socialization practices) and the development of novel algorithms for support of these processes.  We're implementing an algorithmic intervention using the unusual strategy of deploying a set of prediction algorithms as a service and leaving decisions about appropriation to our users and other technology developers.  We are embracing public auditing and re-interpretations of our model's predictions as an intended and desired outcome.  To our knowledge, there is nothing like the intervention we hope to achieve with ORES in the literature and little discussion of the effects that such infrastructural interventions have on communities of practice and their articulation work.

\subsection{Machine prediction in support of open production}
There's a long history of using machine learning in service of efficiency in open peer production systems.  Of greatest relevance to our field site, Wikipedia and related Wikimedia projects, is vandalism detection.  However article quality prediction models have also been explored and applied to help Wikipedians focus their work in the most beneficial places.

\leadin{Vandalism detection.} The problem of damage detecting in Wikipedia is one of great scale.  English Wikipedia receives about 160,000 new edits every day -- each of which could be damaging.  This is due to the characteristics of an open encyclopedia -- when you open up to the world, you let an awful lot of good in with a little bit of bad.  It doesn't really matter how rare a case of vandalism is if your goal is to catch 100\% of it.  If there is any vandalism at all, all edits will need to be reviewed.  When considering this issue as an information overload problem, it is clear that filtering strategies could be hugely successful.

The computer science literature has a wealth of information about how to build effective vandalism detection in Wikipedia.  Starting in 2008, several studies were published that described different methods for detecting vandalism and other damaging edits to articles in Wikipedia.  For example, Potthast et al.'s seminal paper "Automatic Vandalism Detection in Wikipedia\cite{potthast2008automatic} describes a strategy for automatically detecting vandalism with a Logistic Regression and Jacobi described the design of \emph{ClueBot}\cite{carter2008cluebot}, a Wikipedia editing robot designed to automatically revert obvious vandalism.  Years later, Adler et al. (2011) summarized the state of the art in feature extraction and modeling strategies up to that point and build a classifier that aggregated all features extraction strategies to achieve an even higher level of model fitness\cite{adler2011wikipedia}.  While in some cases, researchers directly integrated their prediction models into tools for Wikipedians to use (e.g. STiki\cite{west2010stiki}, a machine-supported human-computation tool), most of this modeling work remains in the literature and has not been incorporated into current tools in Wikipedia.

Still Wikipedians have managed to orchestrate a complex, multi-stage filter for incoming edits that is highly efficient and effective.  Geiger \& Halfaker quantitatively describe the temporal rhythm of edit review in Wikipedia\cite{geiger2013levee}: First, automated revert vandalism that scores very highly according to a machine prediction model, then users of human-computation system augmented by machine prediction models review edits that score highly (but not high enough for the robots).  Edits that make it past these bots and ''cyborgs''\cite{halfaker2012bots} are routed through a system of \emph{watchlists}\footnote{\url{http://enwp.org/:mw:Help:Watchlist}} to experienced Wikipedia editors who are interested in the articles being edited.  With this system in place, most damaging edits are reverted within seconds of when they are saved\cite{geiger2013levee} and Wikipedia is kept clean.

\leadin{Task routing.} Task routing in Wikipedia is largely support by a natural dynamic: people read what they are interested in and when they see an opportunity to contribute, they do.  This process naturally leads to a demand-driven contribution pattern whereby the most viewed content tends to be edited to the highest quality\cite{hill2014consider}.  But there are still many cases where Wikipedia remains misaligned\cite{wang2015misalignment} and content coverage biases creep in (e.g. for a long period of time, the coverage of Women Scientists in Wikipedia lagged far behind the rest of the encyclopedia\cite{halfaker2017interpolating}).  By aligning interests with missed opportunities for contribution these misalignments and gaps can be re-aligned and filled.  Past work has explored collaborative recommender-based task routing strategies (see SuggestBot\cite{cosley2007suggestbot}) and shown good success.  Recently, the maintainers of SuggestBot have developed article quality prediction models to help route attention to low quality articles\cite{wang2013tell}.  Wang and Halfaker have also used the article quality model to perform some one-off analyses to help Wikipedians critique and update their own manual quality assessments\cite{wang2014screening}.

\subsection{Wikipedia's socio-technical problems}
The intersection of how quality is enacted in Wikipedia and how quality control norms and policies are embedded in algorithms and the processes they support has been explored and critiqued by several studies\cite{halfaker2013rise}\cite{morgan2013tea}\cite{halfaker2014snuggle}.  In summary, Wikipedians struggled with the issues of scaling when the popularity of Wikipedia grew exponentially between 2005 and 2007\cite{halfaker2013rise}.  In response, they developed quality control processes and technologies that prioritized efficiency using machine prediction models\cite{halfaker2014snuggle} and templated warning messages\cite{halfaker2013rise}.  The result was that the newcomer socialization experience changed from one that was primarily human and welcoming to one that is more dismissive and impersonal\cite{morgan2013tea}.  This is a good example of the values of the designers being captured in the process and supporting infrastructure they developed\cite{halfaker2014snuggle}.  Put another way, the efficiency of quality control work and the elimination of damage was considered extremely politically important while the positive experience of newcomers was less politically important.  The result was a sudden and sustained decline in the retention of good-faith newcomers and thus a decline in the overall population of Wikipedia editors\cite{halfaker2013rise}.

After the effect of this trade-off was made clear, there have been a number of initiatives started in an effort to more effectively balance the needs for good community management with quality control efficiency.  For example, a newcomer help space (The Teahouse\cite{morgan2013tea}) was developed in order to provide or more welcoming and forgiving space for newcomers to ask questions and meet experienced Wikipedians.  A newcomer training game was developed and tested with financial support from the Wikimedia Foundation\cite{narayan2015effects}.  The Wikimedia Foundation also formed a product development team that was specifically tasked with making changes to Wikipedia's user interfaces that would increase newcomer retention\footnote{\url{http://enwp.org/:m:Growth team}}.  However, most of these efforts did not show gains in newcomer retention under experimental conditions.  The only exception is the Teahouse where it was shown that intervening by inviting newcomers to participate in the question and answer space had a statistically significant benefit to long term newcomer retention\cite{morgan2018evaluating}.

Despite these targeted efforts and shifts in perception among some members of the Wikipedia community, the quality control process that was designed over a decade ago remains largely unchanged\cite{halfaker2014snuggle}.  The quality control systems that were dominant before the publication of the of the seminal "Rise and Decline" study in 2013\cite{halfaker2013rise} remain dominant today.  The regime of automatic reverting bots and semi-automated tools that conceptualize edits as ``good'' and ``bad'' remains in place and unchanged.  Notably, Halfaker et al. experimented with developing a novel reversal of the damage detection tools by using the same model to highlight good editors who were running into trouble in the quality control system\cite{halfaker2014snuggle}, but while the intervention was interesting and caused some reflection and discussion about quality control processes, there has not been a significant shift in how quality control is enacted in Wikipedia.
